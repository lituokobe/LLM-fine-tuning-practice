{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba95acc2-63cd-49e5-b881-d5b0acf7d4ba",
   "metadata": {},
   "source": [
    "## 1. Customize model configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "624748a1-8f06-496f-9c92-6b303ad266b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "43dc6f10-01e4-4b04-b38a-19dbecf223ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Linear(10, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10,2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8ca82767-63be-4194-a88c-dbd1a26ce293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=10, out_features=10, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=10, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ae78ecc4-05cd-49f2-900f-72dd85cb2efd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.modules.container.Sequential"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "07e47cb1-2f2e-4868-8114-8eba33e6f513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.weight\n",
      "0.bias\n",
      "2.weight\n",
      "2.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in net.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ec99f6fd-b553-4c7d-a77d-037ec329a83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LoraConfig(target_modules = [\"0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a4cc4141-956e-47a5-bf84-57798e14f2b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=8, target_modules={'0'}, exclude_modules=None, lora_alpha=8, lora_dropout=0.0, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, target_parameters=None)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "13270b7f-9084-4731-9ddc-63b584951332",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_peft_model(net, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "42c4798d-94f4-4ca8-8558-3744e55775e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Sequential(\n",
       "      (0): lora.Linear(\n",
       "        (base_layer): Linear(in_features=10, out_features=10, bias=True)\n",
       "        (lora_dropout): ModuleDict(\n",
       "          (default): Identity()\n",
       "        )\n",
       "        (lora_A): ModuleDict(\n",
       "          (default): Linear(in_features=10, out_features=8, bias=False)\n",
       "        )\n",
       "        (lora_B): ModuleDict(\n",
       "          (default): Linear(in_features=8, out_features=10, bias=False)\n",
       "        )\n",
       "        (lora_embedding_A): ParameterDict()\n",
       "        (lora_embedding_B): ParameterDict()\n",
       "        (lora_magnitude_vector): ModuleDict()\n",
       "      )\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=10, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9b56da2f-a3be-4cae-b2b8-f4d4f8deceff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 160 || all params: 292 || trainable%: 54.7945\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56b6632-a9b2-4554-b61c-12f5e7e7808b",
   "metadata": {},
   "source": [
    "## 2. Multi-adapter loading and switching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c6034898-f708-4b8d-a154-3bc5ec975ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Linear(10, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10,2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "66c8edbb-e8d0-4ae0-8241-bd7dd0af93c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LoraConfig(target_modules = [\"0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b1868eaf-bd63-43e5-a656-829341bb2740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=8, target_modules={'0'}, exclude_modules=None, lora_alpha=8, lora_dropout=0.0, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, target_parameters=None)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a7f3e76f-3760-469f-8446-4191af21db5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_peft_model(net, config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f2164825-7127-4d4d-86f5-79938505e24d",
   "metadata": {},
   "source": [
    "`get_peft_model` injects adapter layers (e.g., LoRA, IA³, Prompt Tuning) directly into the modules of the base model (net). This means:\n",
    "\n",
    "- The original model object is mutated—not cloned.\n",
    "- The adapter layers are added to the same Python object.\n",
    "- Any reference to net after this call will reflect the modified state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "10b5b693-1b49-4a06-98db-1ee7ee93f6a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): lora.Linear(\n",
       "    (base_layer): Linear(in_features=10, out_features=10, bias=True)\n",
       "    (lora_dropout): ModuleDict(\n",
       "      (default): Identity()\n",
       "    )\n",
       "    (lora_A): ModuleDict(\n",
       "      (default): Linear(in_features=10, out_features=8, bias=False)\n",
       "    )\n",
       "    (lora_B): ModuleDict(\n",
       "      (default): Linear(in_features=8, out_features=10, bias=False)\n",
       "    )\n",
       "    (lora_embedding_A): ParameterDict()\n",
       "    (lora_embedding_B): ParameterDict()\n",
       "    (lora_magnitude_vector): ModuleDict()\n",
       "  )\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=10, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e190221f-972c-4fef-9a05-016566c9871e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Sequential(\n",
       "      (0): lora.Linear(\n",
       "        (base_layer): Linear(in_features=10, out_features=10, bias=True)\n",
       "        (lora_dropout): ModuleDict(\n",
       "          (default): Identity()\n",
       "        )\n",
       "        (lora_A): ModuleDict(\n",
       "          (default): Linear(in_features=10, out_features=8, bias=False)\n",
       "        )\n",
       "        (lora_B): ModuleDict(\n",
       "          (default): Linear(in_features=8, out_features=10, bias=False)\n",
       "        )\n",
       "        (lora_embedding_A): ParameterDict()\n",
       "        (lora_embedding_B): ParameterDict()\n",
       "        (lora_magnitude_vector): ModuleDict()\n",
       "      )\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=10, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15df5ac-eb58-43be-986b-72b881694e26",
   "metadata": {},
   "source": [
    "Assume we finished the training and we are going to save the model.\n",
    "\n",
    "The configuration, not the full model will be saved in the newly created folder designated as an input parameter.\n",
    "\n",
    "When you save a LoRA model using PEFT, you're typically saving:\n",
    "\n",
    "- The LoRA configuration\n",
    "- The LoRA adapter weights (i.e., the low-rank updates)\n",
    "- Not the full pretrained model weights\n",
    "\n",
    "So when you load it back, PEFT needs the original base model (net) to:\n",
    "- Reconstruct the full model by injecting the LoRA layers into the correct places\n",
    "- Ensure the adapter is applied to the correct architecture and modules (e.g., q_proj, v_proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "77f58285-ddd1-4bb9-8323-cfb7b226fa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./LoRA1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "824cec1c-c859-4f2c-9d40-5011a1de90d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=10, out_features=10, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=10, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net2 = nn.Sequential(\n",
    "    nn.Linear(10, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10,2)\n",
    ")\n",
    "net2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "88e1f144-0898-47d1-88a6-efb48a272137",
   "metadata": {},
   "outputs": [],
   "source": [
    "config2 = LoraConfig(target_modules = [\"2\"])\n",
    "model2 = get_peft_model(net2, config2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c19af0f3-e2b9-4ca2-8923-657dc7313821",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save_pretrained(\"./LoRA2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05afe3e3-c4ec-4e48-a1c0-ee8b8022e7ef",
   "metadata": {},
   "source": [
    "Load the saved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "43b557a7-bc87-4c25-a294-5800a4ce3c34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=10, out_features=10, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=10, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Linear(10, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10,2)\n",
    ")\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "73a7b116-7c8e-4755-b63e-0d053943a652",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loaded = PeftModel.from_pretrained(net, model_id = \"./LoRA1\", adapter_name = \"LoRA1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b04d3fef-d45e-43e2-b950-d0b81fbd20c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Sequential(\n",
       "      (0): lora.Linear(\n",
       "        (base_layer): Linear(in_features=10, out_features=10, bias=True)\n",
       "        (lora_dropout): ModuleDict(\n",
       "          (LoRA1): Identity()\n",
       "        )\n",
       "        (lora_A): ModuleDict(\n",
       "          (LoRA1): Linear(in_features=10, out_features=8, bias=False)\n",
       "        )\n",
       "        (lora_B): ModuleDict(\n",
       "          (LoRA1): Linear(in_features=8, out_features=10, bias=False)\n",
       "        )\n",
       "        (lora_embedding_A): ParameterDict()\n",
       "        (lora_embedding_B): ParameterDict()\n",
       "        (lora_magnitude_vector): ModuleDict()\n",
       "      )\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=10, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2c7070b0-4c26-45c8-9d9a-76f4ef388bee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Sequential(\n",
       "      (0): lora.Linear(\n",
       "        (base_layer): Linear(in_features=10, out_features=10, bias=True)\n",
       "        (lora_dropout): ModuleDict(\n",
       "          (default): Identity()\n",
       "        )\n",
       "        (lora_A): ModuleDict(\n",
       "          (default): Linear(in_features=10, out_features=8, bias=False)\n",
       "        )\n",
       "        (lora_B): ModuleDict(\n",
       "          (default): Linear(in_features=8, out_features=10, bias=False)\n",
       "        )\n",
       "        (lora_embedding_A): ParameterDict()\n",
       "        (lora_embedding_B): ParameterDict()\n",
       "        (lora_magnitude_vector): ModuleDict()\n",
       "      )\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=10, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567e58d6-409b-4e24-b246-88b5552e0ab9",
   "metadata": {},
   "source": [
    "Use `load_adapter` to add adapterto existing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "98eb6352-ad94-4e82-93f7-ae01431ca8d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_loaded.load_adapter(\"./LoRA2\", adapter_name = \"LoRA2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "052e8dcd-ab76-428a-a770-de89af0e7337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Sequential(\n",
       "      (0): lora.Linear(\n",
       "        (base_layer): Linear(in_features=10, out_features=10, bias=True)\n",
       "        (lora_dropout): ModuleDict(\n",
       "          (LoRA1): Identity()\n",
       "        )\n",
       "        (lora_A): ModuleDict(\n",
       "          (LoRA1): Linear(in_features=10, out_features=8, bias=False)\n",
       "        )\n",
       "        (lora_B): ModuleDict(\n",
       "          (LoRA1): Linear(in_features=8, out_features=10, bias=False)\n",
       "        )\n",
       "        (lora_embedding_A): ParameterDict()\n",
       "        (lora_embedding_B): ParameterDict()\n",
       "        (lora_magnitude_vector): ModuleDict()\n",
       "      )\n",
       "      (1): ReLU()\n",
       "      (2): lora.Linear(\n",
       "        (base_layer): Linear(in_features=10, out_features=2, bias=True)\n",
       "        (lora_dropout): ModuleDict(\n",
       "          (LoRA2): Identity()\n",
       "        )\n",
       "        (lora_A): ModuleDict(\n",
       "          (LoRA2): Linear(in_features=10, out_features=8, bias=False)\n",
       "        )\n",
       "        (lora_B): ModuleDict(\n",
       "          (LoRA2): Linear(in_features=8, out_features=2, bias=False)\n",
       "        )\n",
       "        (lora_embedding_A): ParameterDict()\n",
       "        (lora_embedding_B): ParameterDict()\n",
       "        (lora_magnitude_vector): ModuleDict()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2e57755b-a177-47ea-bd5c-0e366291cb05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LoRA1'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_loaded.active_adapter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e4087a-0ea1-4913-bc5d-181a2cbee31b",
   "metadata": {},
   "source": [
    "At this time, model_loaded is loaded with LoRA1 and LoRA2, but it only show LoRA1 here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d423eb6c-c7ed-4c9b-bfbe-18a99bb58a2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.2643, -2.1980]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_loaded(torch.arange(0,10).view(1,10).float())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed4f970-0d92-47b8-8669-ea6f9c68d9ae",
   "metadata": {},
   "source": [
    "The above result is the forward propagation with only LoRA1. But actually since LoRA1 is not trained, meaning LoRAB in LoRA1 is only zero, so the result will be the same as if there were no LoRA1. In short, LoRA1 doesn't have any impact to the result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5a6d32-61b7-4601-9d44-f5419a8471d7",
   "metadata": {},
   "source": [
    "Observe all the parameters, especailly LoRA1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ee345b6b-f0f2-4fcf-aa3d-04c2f35bbe21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.0.base_layer.weight Parameter containing:\n",
      "tensor([[ 0.2148,  0.2300, -0.0852, -0.0385,  0.1528, -0.2517,  0.1989,  0.1850,\n",
      "         -0.1537, -0.1598],\n",
      "        [ 0.0701,  0.1910,  0.2447,  0.2428, -0.1525, -0.2923, -0.2148, -0.2323,\n",
      "         -0.1457, -0.1323],\n",
      "        [-0.0601, -0.1806,  0.1027, -0.1469, -0.2808,  0.2860,  0.1875, -0.2528,\n",
      "          0.2934, -0.2301],\n",
      "        [ 0.2359, -0.1629, -0.2355, -0.1299,  0.0024,  0.0470,  0.1063, -0.3047,\n",
      "          0.1232, -0.0876],\n",
      "        [-0.0858, -0.0102,  0.0842,  0.0698,  0.1710, -0.0244, -0.2627,  0.2815,\n",
      "         -0.0844, -0.1858],\n",
      "        [ 0.2067, -0.1552,  0.1700, -0.0743,  0.1910,  0.0698,  0.1104,  0.0090,\n",
      "         -0.0640, -0.1812],\n",
      "        [ 0.0788, -0.2592,  0.1500,  0.2340,  0.2772,  0.3008, -0.2469, -0.3135,\n",
      "         -0.1386, -0.0610],\n",
      "        [-0.0743,  0.3157,  0.2174, -0.2338,  0.2727, -0.2137,  0.1345, -0.2023,\n",
      "         -0.0753, -0.0885],\n",
      "        [ 0.0270,  0.1134,  0.1878,  0.1797,  0.2159,  0.2688,  0.1149,  0.2165,\n",
      "          0.1092,  0.3153],\n",
      "        [ 0.2320,  0.1522,  0.1481,  0.2746, -0.2349, -0.0023,  0.2905,  0.1241,\n",
      "         -0.2798, -0.3116]])\n",
      "base_model.model.0.base_layer.bias Parameter containing:\n",
      "tensor([ 0.2453, -0.0010, -0.2319, -0.2849,  0.0527, -0.0426, -0.1974, -0.0580,\n",
      "        -0.1467,  0.0682])\n",
      "base_model.model.0.lora_A.LoRA1.weight Parameter containing:\n",
      "tensor([[-0.0683, -0.2132,  0.1952, -0.3047,  0.2121,  0.0900, -0.0180,  0.0556,\n",
      "         -0.2182, -0.2453],\n",
      "        [ 0.2704,  0.2913, -0.0486,  0.0069,  0.2548,  0.1713, -0.2362, -0.0890,\n",
      "          0.0916, -0.1812],\n",
      "        [-0.1457,  0.3158,  0.1737,  0.3028, -0.2844,  0.3000, -0.2540,  0.2638,\n",
      "          0.0311,  0.2032],\n",
      "        [-0.0142,  0.2984,  0.0544, -0.2214,  0.3160, -0.1557, -0.0550, -0.2566,\n",
      "          0.1705, -0.0492],\n",
      "        [ 0.1714,  0.0760,  0.1064, -0.3077,  0.0547, -0.1334, -0.2906,  0.0043,\n",
      "          0.1677,  0.2849],\n",
      "        [-0.2482, -0.0563,  0.1417, -0.2779, -0.1537, -0.1336,  0.3114, -0.2974,\n",
      "         -0.1135, -0.1258],\n",
      "        [-0.0480,  0.2389,  0.2661,  0.2964, -0.0054,  0.0292, -0.1821, -0.1194,\n",
      "          0.2797, -0.3101],\n",
      "        [-0.1942, -0.1929, -0.1190,  0.2718,  0.2387,  0.1086, -0.0065,  0.1444,\n",
      "         -0.1001,  0.1666]], requires_grad=True)\n",
      "base_model.model.0.lora_B.LoRA1.weight Parameter containing:\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]], requires_grad=True)\n",
      "base_model.model.2.base_layer.weight Parameter containing:\n",
      "tensor([[ 0.2548,  0.1943,  0.2559, -0.0272,  0.2363,  0.0827,  0.1107,  0.2567,\n",
      "          0.2473,  0.1078],\n",
      "        [-0.1982, -0.2225, -0.3031, -0.2744,  0.1318,  0.1450,  0.2364, -0.3101,\n",
      "         -0.2558, -0.2703]])\n",
      "base_model.model.2.base_layer.bias Parameter containing:\n",
      "tensor([0.0374, 0.1055])\n",
      "base_model.model.2.lora_A.LoRA2.weight Parameter containing:\n",
      "tensor([[-0.2916,  0.1804,  0.0145,  0.2141, -0.1067,  0.2774,  0.2115, -0.1264,\n",
      "          0.0075,  0.2304],\n",
      "        [ 0.0622, -0.0954, -0.2506,  0.0936, -0.0601, -0.1802, -0.1741,  0.2688,\n",
      "          0.3003, -0.2350],\n",
      "        [-0.2006,  0.2685,  0.2140, -0.2350, -0.0205,  0.1762, -0.2690,  0.0613,\n",
      "          0.1032,  0.1013],\n",
      "        [ 0.1856,  0.1215, -0.0758,  0.1430, -0.2543, -0.0854,  0.3159, -0.0527,\n",
      "         -0.0524, -0.1075],\n",
      "        [ 0.3069,  0.0872,  0.2218, -0.0231,  0.0137, -0.0065,  0.3096, -0.0364,\n",
      "          0.2577, -0.2383],\n",
      "        [-0.1068,  0.0382,  0.1029,  0.0246,  0.0115, -0.1763, -0.2135, -0.1433,\n",
      "          0.2724,  0.0335],\n",
      "        [-0.0987,  0.2692,  0.1840, -0.2197,  0.2302, -0.1200, -0.3147,  0.2750,\n",
      "          0.1073, -0.0507],\n",
      "        [-0.0738, -0.2781, -0.2309,  0.2715,  0.1393, -0.2043,  0.0076, -0.2711,\n",
      "          0.2162, -0.1520]])\n",
      "base_model.model.2.lora_B.LoRA2.weight Parameter containing:\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model_loaded.named_parameters():\n",
    "    print(name, param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "fef811ad-ca7a-4cf1-8b48-046dd3ef85c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loraA = ''\n",
    "for name, param in model_loaded.named_parameters():\n",
    "    if name == \"base_model.model.0.lora_A.LoRA1.weight\":\n",
    "    # if name == \"base_model.model.0.lora_B.LoRA1.weight\":\n",
    "        test_loraA = param.detach().cpu().numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2a165436-8862-46d7-9740-754cdf1efdc6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.06827665, -0.21315162,  0.19517507, -0.3047136 ,  0.21207996,\n",
       "        0.08997091, -0.01801044,  0.05561654, -0.21824068, -0.24525881,\n",
       "        0.27042082,  0.2912934 , -0.04864258,  0.00690995,  0.2547788 ,\n",
       "        0.17133598, -0.2362363 , -0.0889783 ,  0.09163094, -0.18120694,\n",
       "       -0.14570914,  0.31579918,  0.17370921,  0.30281734, -0.2844441 ,\n",
       "        0.30003157, -0.2540153 ,  0.26382372,  0.03114098,  0.20320229,\n",
       "       -0.01417893,  0.2983798 ,  0.05441581, -0.22140528,  0.31599414,\n",
       "       -0.15568885, -0.0549856 , -0.25660574,  0.1704619 , -0.04924649,\n",
       "        0.17139727,  0.07604048,  0.10635555, -0.3077171 ,  0.05474796,\n",
       "       -0.13341919, -0.29058054,  0.00432354,  0.16773969,  0.28487453,\n",
       "       -0.24817361, -0.05634926,  0.14166848, -0.27790406, -0.15371028,\n",
       "       -0.13360232,  0.31142262, -0.29741767, -0.11348131, -0.12580545,\n",
       "       -0.04801771,  0.2389253 ,  0.26608676,  0.29641527, -0.00535222,\n",
       "        0.02918509, -0.18207073, -0.11940193,  0.2796891 , -0.3100559 ,\n",
       "       -0.19420104, -0.19292066, -0.11895616,  0.27177656,  0.2387013 ,\n",
       "        0.10859574, -0.00646814,  0.1444007 , -0.10007887,  0.16655506],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loraA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4d76d91f-b9d4-4e13-8035-677dd2288b3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(test_loraA, bins=10)\n",
    "plt.title(\"Distribution of LoRA A Weights\")\n",
    "plt.xlabel(\"Weight Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b1cb42-e474-4b72-baf0-87c744c96828",
   "metadata": {},
   "source": [
    "We change all the parameters in LoRA1 to 1, just like we have trained it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "72611add-0080-4066-bef8-f5da9a0d7ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model_loaded.named_parameters():\n",
    "    if name in [\"base_model.model.0.lora_A.LoRA1.weight\", \"base_model.model.0.lora_B.LoRA1.weight\"]:\n",
    "        param.data = torch.ones_like(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "10c07e5e-72cf-40be-bfcf-f60a130615fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=float32)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loraA = ''\n",
    "for name, param in model_loaded.named_parameters():\n",
    "    if name == \"base_model.model.0.lora_A.LoRA1.weight\":\n",
    "    # if name == \"base_model.model.0.lora_B.LoRA1.weight\":\n",
    "        test_loraA = param.detach().cpu().numpy().flatten()\n",
    "test_loraA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b700612-aff0-4f8c-8a87-a38b77a1c545",
   "metadata": {},
   "source": [
    "Now, when we do the propagation, we will have a different result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "2b2196d9-360c-4910-a476-d72401dd5e89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 618.5175, -475.0137]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_loaded(torch.arange(0,10).view(1,10).float())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b265309-4506-4c7c-b825-d66c5e4a65b4",
   "metadata": {},
   "source": [
    "### Swith to LoRA2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a8f9f133-b72d-4b15-add2-d31fca114af4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Sequential(\n",
       "      (0): lora.Linear(\n",
       "        (base_layer): Linear(in_features=10, out_features=10, bias=True)\n",
       "        (lora_dropout): ModuleDict(\n",
       "          (LoRA1): Identity()\n",
       "        )\n",
       "        (lora_A): ModuleDict(\n",
       "          (LoRA1): Linear(in_features=10, out_features=8, bias=False)\n",
       "        )\n",
       "        (lora_B): ModuleDict(\n",
       "          (LoRA1): Linear(in_features=8, out_features=10, bias=False)\n",
       "        )\n",
       "        (lora_embedding_A): ParameterDict()\n",
       "        (lora_embedding_B): ParameterDict()\n",
       "        (lora_magnitude_vector): ModuleDict()\n",
       "      )\n",
       "      (1): ReLU()\n",
       "      (2): lora.Linear(\n",
       "        (base_layer): Linear(in_features=10, out_features=2, bias=True)\n",
       "        (lora_dropout): ModuleDict(\n",
       "          (LoRA2): Identity()\n",
       "        )\n",
       "        (lora_A): ModuleDict(\n",
       "          (LoRA2): Linear(in_features=10, out_features=8, bias=False)\n",
       "        )\n",
       "        (lora_B): ModuleDict(\n",
       "          (LoRA2): Linear(in_features=8, out_features=2, bias=False)\n",
       "        )\n",
       "        (lora_embedding_A): ParameterDict()\n",
       "        (lora_embedding_B): ParameterDict()\n",
       "        (lora_magnitude_vector): ModuleDict()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "27ed5ab0-7b04-49f6-a01b-0e126ed0bc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loaded.set_adapter(\"LoRA2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a6eba986-c4d5-4e21-986e-dcb774cec318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LoRA2'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_loaded.active_adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "6eb6bda8-f8fb-4d5e-adf5-440e8446f93c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.2643, -2.1980]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_loaded(torch.arange(0,10).view(1,10).float())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a18f513-a1a6-4a69-97d1-08ab90d7548d",
   "metadata": {},
   "source": [
    "This value is the same as when I first do forward propagation with non-touched LoRA1. In other words, LoRA2 doesn't have any impact to the result either here as it is not trained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162b97ed-95f0-4640-bff6-9803e9161f5d",
   "metadata": {},
   "source": [
    "## Disable adapters and have the output from original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "21a78c8b-0ad7-493b-a6fd-679f985baec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.2643, -2.1980]])\n"
     ]
    }
   ],
   "source": [
    "with model_loaded.disable_adapter():\n",
    "    print(model_loaded(torch.arange(0,10).view(1,10).float()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7063e8f-2870-4168-8549-4ae447608d8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
